{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second deduplication\n",
    "\n",
    "This notebook begins with **manifestationmeta.tsv,** and moves toward a smaller dataset that *aspires* to contain only one copy of each \"work,\" in [FRBR terminology.](https://en.wikipedia.org/wiki/Functional_Requirements_for_Bibliographic_Records) \n",
    "\n",
    "However, the key word there is \"aspires.\" We actually rely on a probabilistic model that is known to be wrong about 12% of the time. The model predicts the probability that two records are \"the same work,\" using evidence that includes the similarity of their authors and titles in metadata, but also the degree of similarity between *their texts,* as measured through cosine similarity on extracted features.\n",
    "\n",
    "I've set the probability threshold at 66% to be cautious about collapsing works together. So when the model makes an error it will usually (7%) go wrong by saying that two works are different, and more rarely (5%) mistakenly claim they are the same.\n",
    "\n",
    "**Note** this process is not completely reproducible from this notebook alone; it involves 4GB of data about extracted features that I have not uploaded to the GitHub repo. However if you consult **../get_EF** you can see how to download and process that data yourself. Fair warning: it took ~30hrs of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import math, random, pickle\n",
    "import statsmodels.api as sm\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create blocks\n",
    "\n",
    "We start by grouping volumes into \"blocks.\" This is purely a time-reduction step, to avoid useless comparisons of very different volumes. Each block is identified by the first six characters of the author's name.\n",
    "\n",
    "This strategy does unfortunately mean that the first few characters of names become very important, which is why I made some effort to standardize naming in the first deduplication notebook -- moving e.g. \"sir\" and \"mrs\" to the end of the name. More could probably be done here: names like \"Du Maurier\" and \"Van Dyck\" are potentially tricky.\n",
    "\n",
    "We group volumes in \"blocks\" identified by the first six letters of the author's name. But we also group these blocks into 26 larger groups identified by their first initial. The reason for this is that we may need to parallelize processing and divide data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('../manifestationmeta.tsv', sep = '\\t', low_memory = False, index_col = 'docid')\n",
    "\n",
    "blocks = dict()\n",
    "\n",
    "for idx in meta.index:\n",
    "    name = meta.loc[idx, 'author']\n",
    "    if pd.isnull(name) or len(name) < 3:\n",
    "        name = 'nan'\n",
    "    else:\n",
    "        name = unicodedata.normalize('NFC', name.lower())\n",
    "    \n",
    "    if len(name) < 6:\n",
    "        blockcode = name\n",
    "    else:\n",
    "        blockcode = name[0:6]\n",
    "    \n",
    "    initial = blockcode[0]\n",
    "    if not initial.isalpha() or ord(initial) > 128:\n",
    "        initial = 'x'\n",
    "    \n",
    "    if not initial in blocks:\n",
    "        blocks[initial] = dict()\n",
    "    \n",
    "    if not blockcode in blocks[initial]:\n",
    "        blocks[initial][blockcode] = set()\n",
    "    \n",
    "    blocks[initial][blockcode].add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks:  26\n",
      "all codes:  22346\n",
      "all volumes:  176623\n"
     ]
    }
   ],
   "source": [
    "print('blocks: ', len(blocks))\n",
    "allcodes = 0\n",
    "for b, block in blocks.items():\n",
    "    allcodes += len(block)\n",
    "print('all codes: ', allcodes)\n",
    "print('all volumes: ', len(meta.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing up the text data\n",
    "\n",
    "I downloaded the HathiTrust extracted features for these 176,000 volumes and parsed them using **../get_EF/parsefeaturejsons.py** to produce a matrix where each row is a volume, and the top 1000 features are columns. This ends up being 4GB of data, which is a bit whopping to manipulate in pandas. Plus, I may need to parallelize the processing on different machines. So I'm going to divvy up the matrix.\n",
    "\n",
    "While I'm doing that, I'm also going to sneakily do a couple of other things. First, I'm going to center and scale each of these matrices. (I.e., subtract column mean from each column, and divide by stddev.) The matrices won't all have exactly the same scale, but I don't think that's mission-critical.\n",
    "\n",
    "Second, I'm going to add \"group\" rows to the matrices in cases where a volume belongs to a multi-volume record. This is a tricky aspect of textual similarity. Say I'm comparing a one-volume Middlemarch from 1960 to a 3-volume edition in 1881. \"Oh,\" my program says, \"this 1960 volume doesn't match the first volume of 1881.\" Well, no, of course it doesn't, because that's just the first volume, duh! To avoid that problem, we need to create a second row that sums all the evidence for the volumes. Here we do that by taking the mean of the volumes — since we're looking at frequencies rather than absolute counts, that's roughly adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s (15892, 1000)\n",
      "(17604, 1000)\n",
      "\n",
      "p (6971, 1000)\n",
      "(7686, 1000)\n",
      "\n",
      "d (10052, 1000)\n",
      "(11092, 1000)\n",
      "\n",
      "q (370, 1000)\n",
      "(385, 1000)\n",
      "\n",
      "o (3011, 1000)\n",
      "(3398, 1000)\n",
      "\n",
      "x (611, 1000)\n",
      "(668, 1000)\n",
      "\n",
      "y (919, 1000)\n",
      "(1040, 1000)\n",
      "\n",
      "i (1017, 1000)\n",
      "(1117, 1000)\n",
      "\n",
      "e (3700, 1000)\n",
      "(4232, 1000)\n",
      "\n",
      "w (8923, 1000)\n",
      "(9658, 1000)\n",
      "\n",
      "v (1881, 1000)\n",
      "(1968, 1000)\n",
      "\n",
      "k (5529, 1000)\n",
      "(5924, 1000)\n",
      "\n",
      "n (11023, 1000)\n",
      "(12358, 1000)\n",
      "\n",
      "r (6713, 1000)\n",
      "(7529, 1000)\n",
      "\n",
      "t (7156, 1000)\n",
      "(8209, 1000)\n",
      "\n",
      "l (8268, 1000)\n",
      "(9291, 1000)\n",
      "\n",
      "m (14806, 1000)\n",
      "(16348, 1000)\n",
      "\n",
      "c (12850, 1000)\n",
      "(14213, 1000)\n",
      "\n",
      "z (663, 1000)\n",
      "(689, 1000)\n",
      "\n",
      "h (12699, 1000)\n",
      "(13735, 1000)\n",
      "\n",
      "a (6517, 1000)\n",
      "(6948, 1000)\n",
      "\n",
      "u (449, 1000)\n",
      "(456, 1000)\n",
      "\n",
      "f (6309, 1000)\n",
      "(6949, 1000)\n",
      "\n",
      "b (16544, 1000)\n",
      "(18054, 1000)\n",
      "\n",
      "g (8632, 1000)\n",
      "(9529, 1000)\n",
      "\n",
      "j (3996, 1000)\n",
      "(4420, 1000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix1 = pd.read_csv('/Volumes/TARDIS/work/ef/ficmatrix/featurematrix1.csv', index_col = 'docid')\n",
    "matrix2 = pd.read_csv('../data/featurematrix.csv', index_col = 'docid')\n",
    "inmat1 = set(matrix1.index)\n",
    "inmat2 = set(matrix2.index)\n",
    "\n",
    "def probablymatch(str1, str2):\n",
    "    \n",
    "    m = SequenceMatcher(None, str1, str2)\n",
    "    match = m.real_quick_ratio()\n",
    "    if match > 0.75:\n",
    "        match = m.ratio()\n",
    "    \n",
    "    return match\n",
    "\n",
    "def find_groups(df):\n",
    "    global meta\n",
    "    groupvecs = dict()\n",
    "    \n",
    "    for d in df.index:\n",
    "        record = int(meta.loc[d, 'recordid'])\n",
    "        title = str(meta.loc[d, 'shorttitle'])\n",
    "        thisrec = meta.loc[meta.recordid == record, : ]\n",
    "        matching = []\n",
    "        \n",
    "        for idx in thisrec.index:\n",
    "            thistitle = str(thisrec.loc[idx, 'shorttitle'])\n",
    "            if thistitle == title or probablymatch(thistitle, title) > 0.9:\n",
    "                matching.append(idx)\n",
    "\n",
    "        if len(matching) > 1 and len(matching) < 6:\n",
    "            matchvec = df.loc[matching, : ].mean(axis = 0)\n",
    "            newidx = d + \"group\"\n",
    "            groupvecs[newidx] = matchvec\n",
    "    \n",
    "    return pd.DataFrame.from_dict(groupvecs, orient = 'index')\n",
    "\n",
    "for initial, block in blocks.items():\n",
    "    allvols = set()\n",
    "    for code, vols in block.items():\n",
    "        allvols = allvols.union(vols)\n",
    "    group1 = allvols.intersection(inmat1)\n",
    "    df1 = matrix1.loc[group1, : ]\n",
    "    group2 = allvols.intersection(inmat2)\n",
    "    df2 = matrix2.loc[group2, : ]\n",
    "    df = pd.concat([df1, df2])\n",
    "    print(initial, df.shape)\n",
    "    \n",
    "    # let's scale the matrix\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "    scaled = scaler.transform(df)\n",
    "    df = pd.DataFrame(scaled, index = df.index)\n",
    "    \n",
    "    # augment the matrix with group rows\n",
    "    groupeddf = find_groups(df)\n",
    "    df = pd.concat([df, groupeddf])\n",
    "    print(df.shape)\n",
    "    print()\n",
    "    outfile = '/Volumes/TARDIS/work/ef/ficmatrix/matrix_' + initial + '.csv'\n",
    "    df.to_csv(outfile)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We need to train a model of similarity between volumes, and save the model.\n",
    "\n",
    "Let's start by reading in the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.321825\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            groundtruth   No. Observations:                 1109\n",
      "Model:                          Logit   Df Residuals:                     1106\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Sat, 05 May 2018   Pseudo R-squ.:                  0.4984\n",
      "Time:                        17:00:28   Log-Likelihood:                -356.90\n",
      "converged:                       True   LL-Null:                       -711.53\n",
      "                                        LLR p-value:                9.724e-155\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "titlematch     3.1700      0.177     17.937      0.000       2.824       3.516\n",
      "cossim        -7.8491      0.531    -14.775      0.000      -8.890      -6.808\n",
      "hasworks      -2.9667      0.388     -7.651      0.000      -3.727      -2.207\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('fulltrainingdata.tsv', sep = '\\t')\n",
    "X = data[['titlematch', 'cossim', 'hasworks']]\n",
    "y = data['groundtruth']\n",
    "\n",
    "# Now actually train the model\n",
    "\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature interpretation\n",
    "\n",
    "**titlematch** is the fuzzy similarity between the titles of two volumes\n",
    "**cossim** is the cosine similarity (or really divergence) between their texts\n",
    "**hasworks** is a binary variable, either 0 or 1. It's 1 for comparisons where either title contains the word \"works.\" This turned out to be very important, because we usually don't want to consider volumes of \"Collected Works\" as a match (if they lack shorter titles), and there are a lot of such volumes.\n",
    "\n",
    "#### save to file\n",
    "\n",
    "Now let's save the result to file, so that we can load it later if needed. Statsmodels have a convenient \"save\" function, so that we don't need to use the pickle module directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.save('logistic_model_of_similarity.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test    0.902612\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.DataFrame({'titlematch': 0.95, 'cossim': .1, 'hasworks': 0}, index = ['test'], dtype = 'float')\n",
    "testdf = testdf[['titlematch', 'cossim', 'hasworks']]\n",
    "print(result.predict(testdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix1\n",
    "del matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a block and check for matches\n",
    "\n",
    "Generally, the strategy here is to loop through each block, comparing each record to all the other records in the block. For each comparison, we first check a few basic thresholds (author similarity and title similarity must be > 0.8). If the connection passes those thresholds, we pass title similarity, cosine similarity of texts, and \"hasworks\" to the model.\n",
    "\n",
    "For each volume, we keep a record of all the other volumes that match it. We can later transform this dictionary of *edges* into a list of *connected components*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probablymatch(str1, str2):\n",
    "    '''Runs a quick check, and a better check if the upper bound on\n",
    "    quick check suggests a better check is needed.'''\n",
    "    m = SequenceMatcher(None, str1, str2)\n",
    "    match = m.real_quick_ratio()\n",
    "    if match > 0.75:\n",
    "        match = m.ratio()\n",
    "    \n",
    "    return match\n",
    "\n",
    "def cleanstring(astring, cap):\n",
    "    astring = astring.replace(';', '')\n",
    "    astring = astring.replace(':', '')\n",
    "    astring = astring.lower()\n",
    "    if len(astring) > cap:\n",
    "        astring = astring[0 : cap]\n",
    "    return astring\n",
    "\n",
    "def has_works(title1, title2):\n",
    "    ''' Returns 1 if either title in a pair has the word \"works\".'''\n",
    "    words1 = title1.lower().split()\n",
    "    words1 = [x.strip(',. ') for x in words1]\n",
    "    words2 = title2.lower().split()\n",
    "    words2 = [x.strip(',. ') for x in words2]\n",
    "    \n",
    "    if 'works' in words1 or 'works' in words2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_cossim(doc1, doc2, df, inmatrix):\n",
    "    ''' Calculates cosine similarity between two volumes, and between the\n",
    "    larger groups of vols they belong to, if those groups exist.\n",
    "    '''\n",
    "    if doc1 in inmatrix and doc2 in inmatrix:\n",
    "        vec1 = df.loc[doc1, : ]\n",
    "        vec2 = df.loc[doc2, : ]\n",
    "        cossimA = spatial.distance.cosine(vec1, vec2)\n",
    "        \n",
    "        doc1groupidx = doc1 + 'group'\n",
    "        doc2groupidx = doc2 + 'group'\n",
    "        \n",
    "        if doc1groupidx in inmatrix:\n",
    "            grouped1 = df.loc[doc1groupidx, : ]\n",
    "            cossimB = spatial.distance.cosine(grouped1, vec2)\n",
    "        else:\n",
    "            cossimB = 100\n",
    "        \n",
    "        if doc2groupidx in inmatrix:\n",
    "            grouped2 = df.loc[doc2groupidx, : ]\n",
    "            cossimC = spatial.distance.cosine(vec1, grouped2)\n",
    "        else:\n",
    "            cossimC = 100\n",
    "        \n",
    "        if cossimB < 100 and cossimC < 100:\n",
    "            cossimD = spatial.distance.cosine(grouped1, grouped2)\n",
    "        else:\n",
    "            cossimD = 100\n",
    "        \n",
    "        cossim = min(cossimA, cossimB, cossimC, cossimD)\n",
    "        \n",
    "    else:\n",
    "        cossim = 0.2151\n",
    "        # This was the mean in our training set, and will be used in\n",
    "        # place of NA for comparisons where either vol is missing.\n",
    "    \n",
    "    return cossim\n",
    "    \n",
    "def get_matches(initial, blocks, model):\n",
    "    \n",
    "    block = blocks[initial]\n",
    "    \n",
    "    # get the text data for this block\n",
    "    dataname = '/Volumes/TARDIS/work/ef/ficmatrix/matrix_' + initial + '.csv'\n",
    "    textmatrix = pd.read_csv(dataname, index_col = 'docid')\n",
    "    inmatrix = set(textmatrix.index)\n",
    "    \n",
    "    matches = dict()\n",
    "    repeats = 0\n",
    "    \n",
    "    for code, volset in block.items():\n",
    "        \n",
    "        vols = list(volset)\n",
    "        \n",
    "        already_checked = dict()\n",
    "        titledict = dict()\n",
    "        authdict = dict()\n",
    "    \n",
    "        # we clean all the titles and authors in the vols before \n",
    "        # attempting to match; otherwise you end up doing\n",
    "        # n x n cleaning operations.\n",
    "        \n",
    "        # we also initialize matches\n",
    "    \n",
    "        for b in vols:\n",
    "            if b not in matches:\n",
    "                matches[b] = set()\n",
    "            else:\n",
    "                repeats += 1\n",
    "                # that shouldn't happen\n",
    "                \n",
    "            auth = meta.loc[b, 'author']\n",
    "            if pd.isnull(auth) or len(auth) < 3:\n",
    "                auth = 'cannot-match'\n",
    "            else:\n",
    "                auth = cleanstring(auth, 25)\n",
    "\n",
    "            title = meta.loc[b, 'shorttitle']\n",
    "            if pd.isnull(title) or len(title) < 3:\n",
    "                title = 'cannot-match'\n",
    "            else:\n",
    "                title = cleanstring(title, 35)\n",
    "\n",
    "            titledict[b] = title\n",
    "            authdict[b] = auth\n",
    "\n",
    "        for idx, b1 in enumerate(vols):\n",
    "            \n",
    "            for b2 in vols[idx + 1 : ]:\n",
    "                \n",
    "                auth1 = authdict[b1]\n",
    "                auth2 = authdict[b2]\n",
    "                title1 = titledict[b1]\n",
    "                title2 = titledict[b2]\n",
    "\n",
    "                if auth1 == 'cannot-match' or auth2 == 'cannot-match':\n",
    "                    continue\n",
    "                    \n",
    "                if title1 == 'cannot-match' or title2 == 'cannot-match':\n",
    "                    continue\n",
    "\n",
    "                if auth1 == auth2:\n",
    "                    authormatch = 1.0\n",
    "                else:\n",
    "                    authormatch = probablymatch(auth1, auth2)\n",
    "                    if authormatch < 0.8:\n",
    "                        # we insist on more similarity in authors\n",
    "                        continue\n",
    "\n",
    "                if title1 == title2:\n",
    "                    titlematch = 1.0\n",
    "                else:\n",
    "                    titlematch = probablymatch(title1, title2)\n",
    "                    if titlematch < 0.8:\n",
    "                        # we insist on more similarity in titles\n",
    "                        continue\n",
    "\n",
    "                cossim = calculate_cossim(b1, b2, textmatrix, inmatrix)\n",
    "                hasworks = has_works(title1, title2)\n",
    "\n",
    "                testdf = pd.DataFrame({'titlematch': titlematch, 'cossim': cossim, 'hasworks': hasworks}, index = ['test'], dtype = 'float64')\n",
    "                testdf = testdf[['titlematch', 'cossim', 'hasworks']]\n",
    "                probability = float(model.predict(testdf))\n",
    "                \n",
    "                if probability < 0.66:\n",
    "                    continue\n",
    "                else:\n",
    "                    matches[b1].add(b2)\n",
    "                    matches[b2].add(b1)\n",
    "                    \n",
    "    if repeats > 0:\n",
    "        print('repeats ', repeats)\n",
    "    return matches              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect components\n",
    "\n",
    "The previous function gave us a dictionary where each volume is linked to a set of volumes that match it. This is in essence a data structure of *edges* in a graph.\n",
    "\n",
    "Now we need to transform that structure into a list of *connected components*. Basically, like so:\n",
    "\n",
    "![caption](files/connected.png)\n",
    "\n",
    "Image credit: [Sebastian Thomas.](https://www.mathworks.com/matlabcentral/fileexchange/46457-splitting-a-network-into-connected-components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "def dfs(vertex, matchdict, visited, components, component_ctr):\n",
    "    ''' Depth-first search algorithm. '''\n",
    "    visited.add(vertex)\n",
    "    components[component_ctr].add(vertex)\n",
    "    for link in matchdict[vertex]:\n",
    "        if link not in visited:\n",
    "            dfs(link, matchdict, visited, components, component_ctr)\n",
    "            \n",
    "def connect_components(matchdict):\n",
    "    ''' Visit each vertex. If not yet visited, create a new component, and do \n",
    "    depth-first search on the vertex, adding all linked vertices to the new\n",
    "    component.\n",
    "    '''\n",
    "    \n",
    "    visited = set()\n",
    "    components = []\n",
    "    component_ctr = 0\n",
    "    \n",
    "    for vertex, links in matchdict.items():\n",
    "        if vertex not in visited:\n",
    "            components.append(set())\n",
    "            dfs(vertex, matchdict, visited, components, component_ctr)\n",
    "            component_ctr += 1\n",
    "    \n",
    "    return components\n",
    "    \n",
    "matches = get_matches('q', blocks, result)\n",
    "components = connect_components(matches)\n",
    "print(len(components))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q',\n",
       " 'u',\n",
       " 'y',\n",
       " 'z',\n",
       " 'i',\n",
       " 'x',\n",
       " 'j',\n",
       " 'v',\n",
       " 'o',\n",
       " 'e',\n",
       " 'n',\n",
       " 'f',\n",
       " 'w',\n",
       " 't',\n",
       " 'r',\n",
       " 'p',\n",
       " 'a',\n",
       " 'k',\n",
       " 'g',\n",
       " 'l',\n",
       " 'd',\n",
       " 'h',\n",
       " 'c',\n",
       " 'b',\n",
       " 'm',\n",
       " 's']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialist = []\n",
    "for initial, block in blocks.items():\n",
    "    initialist.append((len(block), initial))\n",
    "\n",
    "initialist.sort()\n",
    "initialist = [x[1] for x in initialist]\n",
    "initialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q 62\n",
      "285\n",
      "u 135\n",
      "680\n",
      "y 148\n",
      "1365\n",
      "z 175\n",
      "1889\n",
      "i 245\n",
      "2673\n",
      "x 353\n",
      "3198\n",
      "j 358\n",
      "5938\n",
      "v 460\n",
      "7504\n",
      "o 538\n",
      "9845\n",
      "e 547\n",
      "12438\n",
      "n 548\n",
      "23022\n",
      "f 730\n",
      "27781\n",
      "w 777\n",
      "34446\n",
      "t 853\n",
      "39084\n",
      "r 953\n",
      "44085\n",
      "p 1025\n",
      "49506\n",
      "a 1038\n",
      "54336\n",
      "k 1096\n",
      "58583\n",
      "g 1115\n",
      "65073\n",
      "l 1177\n",
      "70857\n",
      "d 1233\n",
      "77571\n",
      "h 1297\n",
      "86565\n",
      "c 1450\n",
      "95713\n",
      "b 1988\n",
      "108002\n",
      "m 1992\n",
      "119216\n",
      "s 2053\n",
      "130463\n"
     ]
    }
   ],
   "source": [
    "components = []\n",
    "for initial in initialist:\n",
    "    print(initial, len(blocks[initial]))\n",
    "    matches = get_matches(initial, blocks, result)\n",
    "    components.extend(connect_components(matches))\n",
    "    print(len(components))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little exploratory description\n",
    "\n",
    "E.g., how many groups do we have? How big is the biggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 130463 different components.\n",
      "Of which the biggest contains 483 vols.\n",
      "oldauthor                                           Scott, Walter\n",
      "author                                         Scott, Walter, Sir\n",
      "authordate                                             1771-1832.\n",
      "inferreddate                                                 1829\n",
      "latestcomp                                                   1829\n",
      "datetype                                                        m\n",
      "startdate                                                    1829\n",
      "enddate                                                      1833\n",
      "imprint                              Edinburgh;R. Cadell;1829-33.\n",
      "imprintdate                                             1829-1833\n",
      "contents        XLVIII. [Castle Dangerous] The surgeon's daugh...\n",
      "genres                                                    Fiction\n",
      "subjects                                                      NaN\n",
      "geographics                                                   NaN\n",
      "locnum                                           PZ3.S43A 1839-33\n",
      "oclc                                                     43799838\n",
      "place                                                         stk\n",
      "recordid                                                   379244\n",
      "enumcron                                                     v.32\n",
      "volnum                                                         32\n",
      "title                                            Waverley novels.\n",
      "parttitle                                                     NaN\n",
      "shorttitle                                        Waverley novels\n",
      "instances                                                       1\n",
      "Name: mdp.39015031453205, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('We have ' + str(len(components)) + \" different components.\")\n",
    "\n",
    "maxsize = 0\n",
    "for c in components:\n",
    "    if len(c) > maxsize:\n",
    "        maxsize = len(c)\n",
    "        for ex_biggest in c:\n",
    "            break\n",
    "print(\"Of which the biggest contains \" + str(maxsize) + \" vols.\")\n",
    "print(meta.loc[ex_biggest, : ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the actual deduplication\n",
    "\n",
    "In principle, generally, we want to take one volume from each group of volumes that have matching titles and authors. And in general we want to take the earliest volume, so our resulting dataset will be dated as close as possible to dates of first publication.\n",
    "\n",
    "However, there are complicating cases. What if, for instance, the earliest instance of a novel is a Victorian three-decker edition? That's going to happen pretty often. In that case, we don't want to take *just one volume* from the group; we want all three volumes of the earliest edition. So we need a new rule: take all volumes sharing the *recordid* of the earliest volume. That will get all three volumes of a three-volume edition.\n",
    "\n",
    "But we confront yet another complication! Volumes grouped by a recordid are sometimes three volumes of a single work. But often they are, say, 28 volumes in the *Collected Works of Scott.* All sharing a single record id, but not all the same fictional work. Maybe some of the longer novels are spread across 2 or three volumes, but many of the volumes represent a single novel. This gets bloody complicated.\n",
    "\n",
    "So our *new* rule is: find the earliest volume. Get its record id. Find all volumes sharing that record id (all volumes in the same set). Then take all the volumes that share the same *short title*. If we have been able to identify vols 11 and 12 as *Ivanhoe,* this will get just 11 and 12. However, if we haven't been able to identify titles beyond *Collected Works of Scott,* we'll get all 28 vols! So the final rule is, ignore cases where we recover more than five vols sharing the same recordid. We suspect these are collected works.\n",
    "\n",
    "As we do this, we are going to want to keep track of the number of copies of a volume that have been collapsed into a single deduplicated record. We'll use a column of \"instances\" created in the earlier stage of deduplication; this counts vols that had the same recordid+volnum. We'll further aggregate that into \"copies\": vols that had the same author/title. Moreover, since we may want to distinguish *contemporary* popularity from later canonicity, we're going to keep track of this in two different ways: a general column of copies and a column of copies-published-within-25-yrs of our first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10001\n",
      "20001\n",
      "30001\n",
      "40001\n",
      "50001\n",
      "60001\n",
      "70001\n",
      "80001\n",
      "90001\n",
      "100001\n",
      "110001\n",
      "120001\n",
      "130001\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "selected = []\n",
    "ignored = []\n",
    "errors = 0\n",
    "authtitlecopies = dict()\n",
    "copiesin25yrs = dict()\n",
    "authorsets = []\n",
    "\n",
    "ctr = 0\n",
    "for g in components:\n",
    "    ctr += 1\n",
    "    if ctr % 10000 == 1:\n",
    "        print(ctr)\n",
    "    \n",
    "    # Some groups contain only a single volume.\n",
    "    if len(g) == 1:\n",
    "        for e in g:\n",
    "            break\n",
    "        selected.append(e)\n",
    "        authtitlecopies[e] = int(meta.loc[e, 'instances'])\n",
    "        copiesin25yrs[e] = authtitlecopies[e]\n",
    "        # For a single volume, all these quantities will be the same.\n",
    "        continue\n",
    "        \n",
    "    if len(g) < 1:\n",
    "        errors += 1\n",
    "        continue\n",
    "    \n",
    "    earliest = ''\n",
    "    earliestdate = 2100\n",
    "    instancectr = Counter()\n",
    "    authorset = set()\n",
    "    \n",
    "    for element in g:\n",
    "        date = meta.loc[element, 'inferreddate']\n",
    "        copies = int(meta.loc[element, 'instances'])\n",
    "        auth = meta.loc[element, 'author']\n",
    "        if not pd.isnull(auth):\n",
    "            authorset.add(auth)\n",
    "        \n",
    "        if pd.isnull(date) or int(date) == 0:\n",
    "            date = 2100\n",
    "        else:\n",
    "            date = int(date)\n",
    "        \n",
    "        instancectr[date] += copies\n",
    "        \n",
    "        if earliestdate == 2100 or date < earliestdate:\n",
    "            earliestdate = date\n",
    "            earliest = element\n",
    "            if earliestdate < 1700:\n",
    "                earliestdate = 2100\n",
    "                # don't reward dubious dates\n",
    "                \n",
    "    # different authnames?\n",
    "    if len(authorset) > 1:\n",
    "        authorsets.append(authorset)\n",
    "        \n",
    "    # now let's add up those copies\n",
    "    allcopies = 0\n",
    "    copiesin25yrsofearliest = 0\n",
    "    \n",
    "    for date, count in instancectr.items():\n",
    "        allcopies += count\n",
    "        if date < (earliestdate + 25):\n",
    "            copiesin25yrsofearliest += count\n",
    "            \n",
    "    record = meta.loc[earliest, 'recordid']\n",
    "    title2match = str(meta.loc[earliest, 'shorttitle'])\n",
    "\n",
    "    matching = []\n",
    "\n",
    "    thisrec = meta.loc[meta.recordid == record, : ]\n",
    "    for idx in thisrec.index:\n",
    "        thistitle = str(thisrec.loc[idx, 'shorttitle'])\n",
    "        match = probablymatch(title2match, thistitle)\n",
    "        if match > 0.9:\n",
    "            matching.append(idx)\n",
    "    \n",
    "    if len(matching) < 6:\n",
    "        selected.extend(matching)\n",
    "        for m in matching:\n",
    "            authtitlecopies[m] = allcopies\n",
    "            copiesin25yrs[m] = copiesin25yrsofearliest\n",
    "    else:\n",
    "        ignored.append((title2match, record))\n",
    "        \n",
    "print(errors)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some exploratory description\n",
    "\n",
    "For instance, how many records did we select? How many groups of vols were ignored?\n",
    "\n",
    "Note also that I quietly prune duplicate docids from the **selected** list. The algorithm above permits some duplication to happen, though it's not huge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137017\n",
      "136991\n"
     ]
    }
   ],
   "source": [
    "print(len(selected))\n",
    "\n",
    "# get rid of duplicates\n",
    "selected = list(set(selected))\n",
    "print(len(selected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "print(len(ignored))\n",
    "print(len(authorsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the ignored records to file\n",
    "\n",
    "with open('ignoredgroups.tsv', mode = 'w', encoding = 'utf-8') as f:\n",
    "    for title, record in ignored:\n",
    "        f.write(title + '\\t' + str(record) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also the groups\n",
    "\n",
    "with open('allgroups.tsv', mode = 'w', encoding = 'utf-8') as f:\n",
    "    for g in components:\n",
    "        f.write('\\t'.join(g) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "# And the authorsets\n",
    "\n",
    "authorsets = set([tuple(x) for x in authorsets])\n",
    "print(len(authorsets))\n",
    "# reduce duplication\n",
    "\n",
    "with open('authorsets.tsv', mode = 'w', encoding = 'utf-8') as f:\n",
    "    for s in authorsets:\n",
    "        f.write('\\t'.join(s) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now actually produce and write the dataframe\n",
    "\n",
    "All of our effort so far has gone into selecting a list of indices that will be retained. Now we have to use those indices to actually produce a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like so\n",
    "\n",
    "deduped = meta.loc[selected, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oldauthor</th>\n",
       "      <th>author</th>\n",
       "      <th>authordate</th>\n",
       "      <th>inferreddate</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>datetype</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "      <th>imprint</th>\n",
       "      <th>imprintdate</th>\n",
       "      <th>...</th>\n",
       "      <th>locnum</th>\n",
       "      <th>oclc</th>\n",
       "      <th>place</th>\n",
       "      <th>recordid</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>volnum</th>\n",
       "      <th>title</th>\n",
       "      <th>parttitle</th>\n",
       "      <th>shorttitle</th>\n",
       "      <th>instances</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mdp.39015048438470</th>\n",
       "      <td>White, Hervey</td>\n",
       "      <td>White, Hervey</td>\n",
       "      <td>1866-1944.</td>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>s</td>\n",
       "      <td>1913</td>\n",
       "      <td></td>\n",
       "      <td>Woodstock, N. Y.;The Maverick press;1913.</td>\n",
       "      <td>1913</td>\n",
       "      <td>...</td>\n",
       "      <td>PZ3.W5837Ho</td>\n",
       "      <td>819785</td>\n",
       "      <td>nyu</td>\n",
       "      <td>325093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The house in the road; | a fantasy of truth, |...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The house in the road; a fantasy of truth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pst.000033219158</th>\n",
       "      <td>Tierney, James</td>\n",
       "      <td>Tierney, James</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>s</td>\n",
       "      <td>1997</td>\n",
       "      <td></td>\n",
       "      <td>Merrylands, NSW, Australia|Newman Centre Publi...</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38758319</td>\n",
       "      <td>xx</td>\n",
       "      <td>7509962</td>\n",
       "      <td>T579bushbu 1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bush boys and bush rangers / | $c: by James Ti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bush boys and bush rangers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uc1.b4368903</th>\n",
       "      <td>Tuttle, Anthony</td>\n",
       "      <td>Tuttle, Anthony</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1972</td>\n",
       "      <td>1972</td>\n",
       "      <td>s</td>\n",
       "      <td>1972</td>\n",
       "      <td></td>\n",
       "      <td>Garden City, N.Y.|Doubleday|1972.</td>\n",
       "      <td>1972</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240543</td>\n",
       "      <td>nyu</td>\n",
       "      <td>9510251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Songs from the night before.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Songs from the night before</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uc1.b4357916</th>\n",
       "      <td>Asch, Sholem</td>\n",
       "      <td>Asch, Sholem</td>\n",
       "      <td>1880-1957.</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>s</td>\n",
       "      <td>1946</td>\n",
       "      <td></td>\n",
       "      <td>New York|G. P. Putnam's sons|1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>325381</td>\n",
       "      <td>nyu</td>\n",
       "      <td>983714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>East river, | a novel. | $c: Translation by A....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>East river, a novel</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wu.89044674927</th>\n",
       "      <td>De Wette, Wilhelm Martin Leberecht</td>\n",
       "      <td>De Wette, Wilhelm Martin Leberecht</td>\n",
       "      <td>1780-1849.</td>\n",
       "      <td>1841</td>\n",
       "      <td>1841</td>\n",
       "      <td>s</td>\n",
       "      <td>1841</td>\n",
       "      <td></td>\n",
       "      <td>Boston;Hilliard, Gray;1841.</td>\n",
       "      <td>1841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22633272</td>\n",
       "      <td>mau</td>\n",
       "      <td>8449523</td>\n",
       "      <td>v.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Theodore : | or, The skeptic's conversion / | ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Theodore : or, The skeptic's conversion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             oldauthor  \\\n",
       "docid                                                    \n",
       "mdp.39015048438470                       White, Hervey   \n",
       "pst.000033219158                        Tierney, James   \n",
       "uc1.b4368903                           Tuttle, Anthony   \n",
       "uc1.b4357916                              Asch, Sholem   \n",
       "wu.89044674927      De Wette, Wilhelm Martin Leberecht   \n",
       "\n",
       "                                                author  authordate  \\\n",
       "docid                                                                \n",
       "mdp.39015048438470                       White, Hervey  1866-1944.   \n",
       "pst.000033219158                        Tierney, James         NaN   \n",
       "uc1.b4368903                           Tuttle, Anthony         NaN   \n",
       "uc1.b4357916                              Asch, Sholem  1880-1957.   \n",
       "wu.89044674927      De Wette, Wilhelm Martin Leberecht  1780-1849.   \n",
       "\n",
       "                    inferreddate  latestcomp datetype startdate enddate  \\\n",
       "docid                                                                     \n",
       "mdp.39015048438470          1913        1913        s      1913           \n",
       "pst.000033219158            1997        1997        s      1997           \n",
       "uc1.b4368903                1972        1972        s      1972           \n",
       "uc1.b4357916                1946        1946        s      1946           \n",
       "wu.89044674927              1841        1841        s      1841           \n",
       "\n",
       "                                                              imprint  \\\n",
       "docid                                                                   \n",
       "mdp.39015048438470          Woodstock, N. Y.;The Maverick press;1913.   \n",
       "pst.000033219158    Merrylands, NSW, Australia|Newman Centre Publi...   \n",
       "uc1.b4368903                        Garden City, N.Y.|Doubleday|1972.   \n",
       "uc1.b4357916                        New York|G. P. Putnam's sons|1946   \n",
       "wu.89044674927                            Boston;Hilliard, Gray;1841.   \n",
       "\n",
       "                   imprintdate    ...           locnum      oclc place  \\\n",
       "docid                             ...                                    \n",
       "mdp.39015048438470        1913    ...      PZ3.W5837Ho    819785   nyu   \n",
       "pst.000033219158          1997    ...              NaN  38758319   xx    \n",
       "uc1.b4368903              1972    ...              NaN    240543   nyu   \n",
       "uc1.b4357916              1946    ...              NaN    325381   nyu   \n",
       "wu.89044674927            1841    ...              NaN  22633272   mau   \n",
       "\n",
       "                   recordid         enumcron volnum  \\\n",
       "docid                                                 \n",
       "mdp.39015048438470   325093              NaN    NaN   \n",
       "pst.000033219158    7509962  T579bushbu 1997    NaN   \n",
       "uc1.b4368903        9510251              NaN    NaN   \n",
       "uc1.b4357916         983714              NaN    NaN   \n",
       "wu.89044674927      8449523              v.2    2.0   \n",
       "\n",
       "                                                                title  \\\n",
       "docid                                                                   \n",
       "mdp.39015048438470  The house in the road; | a fantasy of truth, |...   \n",
       "pst.000033219158    Bush boys and bush rangers / | $c: by James Ti...   \n",
       "uc1.b4368903                             Songs from the night before.   \n",
       "uc1.b4357916        East river, | a novel. | $c: Translation by A....   \n",
       "wu.89044674927      Theodore : | or, The skeptic's conversion / | ...   \n",
       "\n",
       "                    parttitle                                 shorttitle  \\\n",
       "docid                                                                      \n",
       "mdp.39015048438470        NaN  The house in the road; a fantasy of truth   \n",
       "pst.000033219158          NaN                 Bush boys and bush rangers   \n",
       "uc1.b4368903              NaN                Songs from the night before   \n",
       "uc1.b4357916              NaN                        East river, a novel   \n",
       "wu.89044674927            NaN    Theodore : or, The skeptic's conversion   \n",
       "\n",
       "                    instances  \n",
       "docid                          \n",
       "mdp.39015048438470          1  \n",
       "pst.000033219158            1  \n",
       "uc1.b4368903                1  \n",
       "uc1.b4357916                3  \n",
       "wu.89044674927              1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add copy counts\n",
    "\n",
    "Before we write out the dataframe, add columns reflecting the number of copies collapsed into each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_count(idx, dictionary):\n",
    "    return dictionary[idx]\n",
    "\n",
    "deduped = deduped.assign(allcopiesofwork = deduped.apply(lambda row: get_copy_count(row.name, authtitlecopies), axis = 1))\n",
    "deduped = deduped.assign(copiesin25yrs = deduped.apply(lambda row: get_copy_count(row.name, copiesin25yrs), axis = 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['oldauthor', 'author', 'authordate', 'inferreddate', 'latestcomp',\n",
      "       'datetype', 'startdate', 'enddate', 'imprint', 'imprintdate',\n",
      "       'contents', 'genres', 'subjects', 'geographics', 'locnum', 'oclc',\n",
      "       'place', 'recordid', 'enumcron', 'volnum', 'title', 'parttitle',\n",
      "       'shorttitle', 'instances', 'allcopiesofwork', 'copiesin25yrs'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(deduped.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort rows\n",
    "deduped.sort_values(by = ['inferreddate', 'recordid', 'volnum'], inplace = True)\n",
    "\n",
    "# put columns in desired order (title last)\n",
    "deduped = deduped[['oldauthor', 'author', 'authordate', 'inferreddate',\n",
    "       'latestcomp', 'datetype', 'startdate', 'enddate', 'imprint',\n",
    "       'imprintdate', 'contents', 'genres', 'subjects', 'geographics',\n",
    "       'locnum', 'oclc', 'place', 'recordid', 'instances', 'allcopiesofwork',\n",
    "       'copiesin25yrs', 'enumcron', 'volnum', 'title',\n",
    "       'parttitle', 'shorttitle']]\n",
    "\n",
    "# write to file\n",
    "deduped.to_csv('newworkmeta.tsv', sep = '\\t', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
